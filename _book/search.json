[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Comparing LLM-Generated Reviews to Human Evaluations in Social Science Research",
    "section": "",
    "text": "Project Overview\nWe are testing the capabilities of current large language models (LLMs), illustrating whether they can generate research paper evaluations comparable to expert human reviews. In this project, we use an AI (OpenAI’s o3 model for now, later o3-pro and Google’s Gemini 2.5 Pro) to review social science research papers under the same criteria used by human reviewers in The Unjournal.\nEach paper is assessed on specific dimensions – for example, the strength of its evidence, rigor of methods, clarity of communication, openness/reproducibility, relevance to global priorities, and overall quality. The LLM will provide quantitative scores (with uncertainty intervals) on these criteria and produce a written evaluation\nOur initial dataset will include research papers that have existing human evaluations. For each paper, the AI will generate: (1) numeric ratings on the defined criteria, (2) identification of the paper’s key claims, and (3) a detailed review discussing the paper’s contributions and weaknesses. We will then compare the AI-generated evaluations to the published human evaluations.\nNext, we will focus on papers currently under evaluation, ie where no human evaluation exists yet and we can rule out any contamination.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Project Overview</span>"
    ]
  },
  {
    "objectID": "numerical_ratings.html",
    "href": "numerical_ratings.html",
    "title": "Quantitative metrics",
    "section": "",
    "text": "Setup\nThe block below\n\nloads the main libraries (OpenAI SDK, Altair, Plotly, …)\nlooks for your OpenAI key in key/openai_key.txt\ninitialises a client (o3 by default – flip to o3-pro for &gt;128 k-token papers)\ndefines pdf_to_string()\n– drops the reference section and truncates at 180 k tokens so we stay in-context.\n\n\n\nShow code\nimport os, pathlib, json, textwrap, pdfplumber, re, pandas as pd, numpy as np\nfrom typing import Dict, Any\nfrom openai import OpenAI\nimport altair as alt\nalt.renderers.enable(\"html\") \nimport plotly.io as pio\nimport plotly.graph_objects as go\n\n# \n# Install if it isn’t present \n# %py -3.13 -m pip install --upgrade openai\n# \n# -------------------------------------------------------------------\n# Locate API key: env var ➜ key/openai_key.txt\n# -------------------------------------------------------------------\nkey_path = pathlib.Path(\"key/openai_key.txt\")\n\nif os.getenv(\"OPENAI_API_KEY\") is None and key_path.exists():\n    os.environ[\"OPENAI_API_KEY\"] = key_path.read_text().strip()\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\n        \"No API key found.\\n\"\n        \"Create key/openai_key.txt (single line) or export OPENAI_API_KEY in your shell.\"\n    )\n\nclient = OpenAI()  # SDK reads the key from the env var\n\n\nmodel = \"o3\"\n# model = \"o3-pro\"\n\n\n\n# ------------------------------\n# PDF → plain‑text utility\n# ------------------------------\nimport re, pdfplumber\n\ndef pdf_to_string(path, max_tokens=180_000, model=\"o3-pro\"):\n    \"\"\"Extract text, drop refs, hard-cap by tokens.\"\"\"\n    import tiktoken\n    enc = tiktoken.encoding_for_model(model)\n\n    with pdfplumber.open(path) as pdf:\n        text = \" \".join(p.extract_text() or \"\" for p in pdf.pages)\n\n    # kill excessive whitespace\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # drop everything after References / Bibliography\n    m = re.search(r\"\\b(References|Bibliography)\\b\", text, flags=re.I)\n    if m:\n        text = text[: m.start()]\n\n    # token-trim\n    tokens = enc.encode(text)\n    if len(tokens) &gt; max_tokens:\n        text = enc.decode(tokens[:max_tokens])\n\n    return text\n\n\n\n\nResponse schema & system prompt\n\nMETRICS lists the seven Unjournal criteria.\n\nresponse_format is a JSON-Schema guard so the model can’t hallucinate keys.\n\nSYSTEM_PROMPT is the rubric we give the LLM.\nNotes:\n• Scores are percentiles vs serious work in the last 3 years.\n• overall defaults to the mean of the other six but the model may override.\nevaluate_paper() wraps everything: PDF → text → chat completion → dict.\n\n\n\nShow code\n# -----------------------------\n# 1.  Metric list\n# -----------------------------\nMETRICS = [\n    \"overall\",\n    \"claims_evidence\",\n    \"methods\",\n    \"advancing_knowledge\",\n    \"logic_communication\",\n    \"open_science\",\n    \"global_relevance\"\n]\n\n# -----------------------------\n# 2.  JSON schema\n# -----------------------------\nmetric_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"midpoint\":    {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100},\n        \"lower_bound\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100},\n        \"upper_bound\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100},\n        \"rationale\":   {\"type\": \"string\"}\n    },\n    \"required\": [\"midpoint\", \"lower_bound\", \"upper_bound\", \"rationale\"],\n    \"additionalProperties\": False\n}\n\nresponse_format = {\n    \"type\": \"json_schema\",\n    \"json_schema\": {\n        \"name\": \"paper_assessment_v1\",\n        \"strict\": True,\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"metrics\": {\n                    \"type\": \"object\",\n                    \"properties\": {m: metric_schema for m in METRICS},\n                    \"required\": METRICS,\n                    \"additionalProperties\": False\n                }\n            },\n            \"required\": [\"metrics\"],\n            \"additionalProperties\": False\n        }\n    }\n}\n\n\n# -----------------------------\n# 3.  System prompt\n# -----------------------------\nSYSTEM_PROMPT = textwrap.dedent(f\"\"\"\nYou are an expert evaluator.\n\nWe ask for a set of nine quantitative metrics. For each metric, we ask for a score and a 90% credible interval.\n\nPercentile rankings\nWe ask for a percentile ranking from 0-100%. This represents \"what proportion of papers in the reference group are worse than this paper, by this criterion\". A score of 100% means this is essentially the best paper in the reference group. 0% is the worst paper. A score of 50% means this is the median paper; i.e., half of all papers in the reference group do this better, and half do this worse, and so on.\nThe population of papers should be all serious research in the same area that you have encountered in the last three years.\n\nMidpoint rating and credible intervals \nFor each metric, we ask you to provide a 'midpoint rating' and a 90% credible interval as a measure of your uncertainty. \nWe want policymakers, researchers, funders, and managers to be able to use evaluations to update their beliefs and make better decisions. Evaluators may feel confident about their rating for one category, but less confident in another area. How much weight should readers give to each? In this context, it is useful to quantify the uncertainty. \nYou are asked to give a 'midpoint' and a 90% credible interval. Consider this as the smallest interval that you believe is 90% likely to contain the true value.\n\nOverall assessment\n- Judge the quality of the research heuristically. Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.importance to knowledge production, and importance to practice. \n\n\nClaims, strength and characterization of evidence\n- Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?\n\nMethods: Justification, reasonableness, validity, robustness\n- Are the methods used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? \n- Are the results and methods likely to be robust to reasonable changes in the underlying assumptions? Does the author demonstrate this?\n- Avoiding bias and questionable research practices (QRP): Did the authors take steps to reduce bias from opportunistic reporting and QRP? For example, did they do a strong pre-registration and pre-analysis plan, incorporate multiple hypothesis testing corrections, and report flexible specifications? \n\nAdvancing our knowledge and practice\n- To what extent does the project contribute to the field or to practice, particularly in ways that are relevant to global priorities and impactful interventions?\n- Do the paper's insights inform our beliefs about important parameters and about the effectiveness of interventions? \n- Does the project add useful value to other impactful research? We don't require surprising results; sound and well-presented null results can also be valuable.\n\n\nLogic and communication\n- Are the goals and questions of the paper clearly expressed? Are concepts clearly defined and referenced?\n- Is the reasoning \"transparent\"? Are assumptions made explicit? Are all logical steps clear and correct? Does the writing make the argument easy to follow?\n- Are the conclusions consistent with the evidence (or formal proofs) presented? Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? \n- Are the data and/or analysis presented relevant to the arguments made? Are the tables, graphs, and diagrams easy to understand in the context of the narrative (e.g., no major errors in labeling)?\n\nOpen, collaborative, replicable research\n- Replicability, reproducibility, data integrity: Would another researcher be able to perform the same analysis and get the same results? Are the methods explained clearly and in enough detail to enable easy and credible replication? For example, are all analyses and statistical tests explained, and is code provided?\n- Is the source of the data clear? Is the data made as available as is reasonably possible? If so, is it clearly labeled and explained?? \n- Consistency: Do the numbers in the paper and/or code output make sense? Are they internally consistent throughout the paper?\n- Useful building blocks: Do the authors provide tools, resources, data, and outputs that might enable or enhance future work and meta-analysis?\n\nRelevance to global priorities, usefulness for practitioners\n- Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? \n- Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic? \n- Do the authors report results that are relevant to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.) enabling practical impact quantification and prioritization? \n- Do they communicate (at least in the abstract or introduction)  in ways policymakers and decision-makers can understand, without misleading or oversimplifying?\n\nReturn STRICT JSON matching the supplied schema.\n\nFill every key in the object `metrics`:\n\n  {', '.join(METRICS)}\n\nDefinitions are percentile scores (0 – 100) versus serious work in the field from the last 3 years.\nFor `overall`:\n  • Default = arithmetic mean of the other six midpoints (rounded).  \n  • If, in your judgment, another value is better (e.g. one metric is far more decision-relevant), choose it **and explain why** in `overall.rationale`.\n\nField meanings\n  midpoint      → best-guess percentile\n  lower_bound   → 5th-percentile plausible value\n  upper_bound   → 95th-percentile plausible value\n  rationale     → ≤100 words; terse but informative.\n\nDo **not** wrap the JSON in markdown fences or add extra text.\n\"\"\").strip()\n\n\ndef evaluate_paper(pdf_path: str | pathlib.Path,\n                   model: str = model) -&gt; dict:\n    paper_text = pdf_to_string(pdf_path)\n\n    chat = client.chat.completions.create(\n        model=model,\n        # temperature=temperature,\n        response_format=response_format,\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\",   \"content\": paper_text}\n        ]\n    )\n\n    raw_json = chat.choices[0].message.content\n    return json.loads(raw_json)\n\n\n\n\nBatch-evaluate all PDFs\nBelow we iterate over every file in papers/, call evaluate_paper(), sleep 1.1 s (rate-limit padding), and store the full JSON per paper in results/.\n\n\nResults\n\nFlattens the nested JSON into long format:\none row = (paper, metric, midpoint, lower, upper, rationale).\n\nSaves results/metrics_long.csv for quick downstream use.\nRegisters an Altair + Plotly “Unjournal” theme so all charts use\n– brand green #99bb66 and accent orange #f19e4b\n\nRidgeline density plot: Kernel-density of mid-points for each metric. Height is normalised, x-axis is 0-100 %. Green fill matches the site palette.\nHeat-map\nInteractive Plotly widget\n\n\n\nShow code\ntidy_rows = []\nfor rec in records:\n    paper_id = rec[\"paper\"]\n    for metric, vals in rec[\"metrics\"].items():\n        tidy_rows.append({\n            \"paper\":   paper_id,\n            \"metric\":  metric,\n            **vals     # midpoint, lower_bound, upper_bound, rationale\n        })\n\ntidy = pd.DataFrame(tidy_rows)\ntidy.to_csv(OUT / \"metrics_long.csv\", index=False)\n# tidy.head()\n\n\nUNJ_GREEN  = \"#99bb66\"   # brand green\nUNJ_ORANGE = \"#f19e4b\"   # accent\n\n# ── 1 ▸ Altair theme ────────────────────────────────────────────\ndef unj_theme():\n    return {\n        \"config\": {\n            \"view\":  {\"stroke\": \"transparent\"},\n            \"background\": \"white\",\n            \"title\": {\"font\": \"Source Sans Pro\", \"fontSize\": 16, \"color\": \"#222\"},\n            \"axis\":  {\"labelFont\": \"Source Sans Pro\", \"titleFont\": \"Source Sans Pro\",\n                      \"labelColor\": \"#222\", \"titleColor\": \"#222\",\n                      \"gridOpacity\": 0.15},\n            \"legend\": {\"labelFont\": \"Source Sans Pro\", \"titleFont\": \"Source Sans Pro\"},\n            # continuous colourbars (heatmaps etc.)\n            \"range\": {\n                \"heatmap\": [\n                    \"#f6fbf3\", \"#e2f1d7\", \"#cfe7ba\", \"#badc9c\",\n                    \"#a6d27f\", \"#92c861\", \"#7dbd43\", \"#69b325\",\n                    \"#55a807\", \"#477b13\"   # darker end\n                ],\n                # default nominal palette starts with green→orange\n                \"category\": [\n                    UNJ_GREEN, UNJ_ORANGE,\n                    \"#6bb0f3\", \"#d9534f\", \"#636363\", \"#ffb400\",\n                    \"#53354a\", \"#2780e3\", \"#3fb618\", \"#8e6c8a\"\n                ]\n            }\n        }\n    }\n\nalt.themes.register(\"unj\", unj_theme)\nalt.themes.enable(\"unj\")   # ← every Altair chart now uses it\n\n# ── 2 ▸ Plotly template ─────────────────────────────────────────\npio.templates[\"unj\"] = go.layout.Template(\n    layout = dict(\n        font=dict(family=\"Source Sans Pro, Helvetica, Arial, sans-serif\",\n                  size=14, color=\"#222\"),\n        colorway=[UNJ_GREEN, UNJ_ORANGE, \"#6bb0f3\", \"#d9534f\", \"#636363\"],\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        xaxis=dict(gridcolor=\"rgba(0,0,0,0.15)\"),\n        yaxis=dict(gridcolor=\"rgba(0,0,0,0.15)\")\n    )\n)\npio.templates.default = \"unj\"\n\n\n\n\n\n\nFigure 2.1: Mid-point score distributions for every metric. Each density is scaled to unit height; x-axis is the percentile scale (0 – 100 %).\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\nif \"short\" not in tidy.columns:\n    def make_short(name: str) -&gt; str:\n        if re.fullmatch(r\"w\\d{5}\", name, re.I):\n            return name.upper()                         # e.g. W30539\n        parts = re.split(r\"[_\\-\\s]+\", name)\n        year  = next((p for p in parts if re.fullmatch(r\"\\d{4}\", p)), \"\")\n        auth  = next((p for p in parts if p and p[0].isalpha()), name)[:12]\n        return f\"{auth.title()} ({year})\" if year else auth.title()\n\n    tidy[\"short\"] = tidy[\"paper\"].apply(make_short)\n\n# ── 1 ▸ reshape & attach short labels ──────────────────────────────\nheat = (tidy\n        .pivot(index='metric', columns='paper', values='midpoint')\n        .loc[METRICS[::-1]])               # keep row order\nheat = (heat\n        .reset_index()\n        .melt(id_vars='metric', var_name='paper', value_name='midpoint')\n        .merge(tidy[['paper','short']].drop_duplicates(), on='paper'))\n\n# ── 2 ▸ order papers by overall mean score (best → worst) ─────────\norder = (tidy.groupby('paper')['midpoint']\n              .mean()\n              .sort_values(ascending=False)\n              .index.to_list())\norder_short = (tidy[['paper','short']]\n               .drop_duplicates()\n               .set_index('paper')\n               .loc[order]['short']\n               .to_list())\n\n# ── 3 ▸ build chart ────────────────────────────────────────────────\nalt.Chart(heat).mark_rect().encode(\n    y = alt.Y('metric:N', sort=None, title=None),\n    x = alt.X('short:N', sort=order_short, title=None),\n    color = alt.Color('midpoint:Q',\n                      scale = alt.Scale(domain=[0,100]),   # uses theme’s green gradient\n                      legend = alt.Legend(title='Score')),\n    tooltip = ['short','metric','midpoint']\n).properties(\n    height = 40 * len(METRICS),\n    width  = 14 * heat['short'].nunique()\n)\n\n\n\n\nFigure 2.2: Per-paper mid-point scores across all metrics. Darker green → higher percentile. Columns ordered by each paper’s overall average.\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# 1 ─── data prep ----------------------------------------------------\nMETRICS_REV = METRICS[::-1]          # want first metric on top\ntidy_sorted = (tidy\n               .assign(metric=pd.Categorical(tidy.metric, categories=METRICS_REV, ordered=True))\n               .sort_values(\"metric\"))\n\npapers = [\"All\"] + sorted(tidy.paper.unique())\n\n# 2 ─── build one trace per paper -----------------------------------\nfig = go.Figure()\n\nfor pap in tidy.paper.unique():\n    dfp = tidy_sorted[tidy_sorted.paper == pap]\n    fig.add_trace(go.Scatter(\n        x=dfp.midpoint,\n        y=dfp.metric,\n        mode=\"markers\",\n        marker=dict(size=10),\n        error_x=dict(\n            type=\"data\",\n            symmetric=False,\n            array=dfp.upper_bound - dfp.midpoint,\n            arrayminus=dfp.midpoint - dfp.lower_bound),\n        name=pap,\n        visible=False      # show later via dropdown\n    ))\n\n# 3 ─── dropdown logic ----------------------------------------------\nbuttons = []\n\n# individual papers\nfor i, pap in enumerate(tidy.paper.unique()):\n    vis = [False] * len(tidy.paper.unique())\n    vis[i] = True\n    buttons.append(dict(\n        label=pap,\n        method=\"update\",\n        args=[{\"visible\": vis},\n              {\"title\": f\"Ratings for paper: {pap}\"}]\n    ))\n\n# 'All' view (dots overlaid)\nbuttons.insert(0, dict(\n    label=\"All\",\n    method=\"update\",\n    args=[{\"visible\": [True]*len(tidy.paper.unique())},\n          {\"title\": \"Ratings for all papers\"}]\n))\n\nfig.update_layout(\n    updatemenus=[dict(\n        buttons=buttons,\n        direction=\"down\",\n        showactive=True,\n        x=0.01, y=1.15\n    )],\n    yaxis=dict(categoryorder=\"array\", categoryarray=METRICS_REV, title=None),\n    xaxis=dict(range=[0, 100], title=\"Percentile score\"),\n    height=40*len(METRICS) + 120,\n    title=\"Ratings for all papers\"\n)\n\nfig\n\n\n\n\nFigure 2.3\n\n\n\n\n\n(a) Interactive inspector: select a single paper to show its mid-point (dot) and 90 % credible interval (whisker) for every metric.\n\n\n        \n        \n        \n\n\n\n\n\n\n(b)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quantitative metrics</span>"
    ]
  },
  {
    "objectID": "compare_ratings.html",
    "href": "compare_ratings.html",
    "title": "LLM vs. Human Ratings",
    "section": "",
    "text": "Overall correlation\nTable 3.1: Sample size and mean absolute difference (MAD) by metric.\n\n\n\n\n\n\n\n\n\n \nN\nMAD\n\n\nmetric\n \n \n\n\n\n\nadvancing_knowledge\n23\n10.8\n\n\nclaims_evidence\n5\n14.7\n\n\nglobal_relevance\n24\n14.8\n\n\nlogic_communication\n24\n13.1\n\n\nmethods\n23\n14.2\n\n\nopen_science\n24\n20.7\n\n\noverall\n24\n11.3\nFigure 3.1: LLM mid-points vs. human mid-points. Dashed 45° = exact agreement.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM vs. Human Ratings</span>"
    ]
  }
]